{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3cc9d2b",
   "metadata": {},
   "source": [
    "# Project 4: Machine Learning Fairness Algorithms Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb6364",
   "metadata": {},
   "source": [
    "##  Outline\n",
    "\n",
    "* Part 1: To introduction Algorithms method A2(**Maximizing accuracy under fairness constraints (C-SVM and C-LR)**) and A7(**Information Theoretic Measures for Fairness-aware Feature selection (FFS)**).\n",
    "\n",
    "* Part 2: To show how the evaluation was carried out\n",
    "\n",
    "* Part 3: To show the main results from these two methods with Fairness theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd70f28",
   "metadata": {},
   "source": [
    "### Preprocessing the data and modules\n",
    "\n",
    "The data here we used is from **COMPAS Dataset**.\n",
    "\n",
    "Moreover, in this project, we selected 'race', 'sex', 'age', 'juv_misd_count' and 'priors_count' as the features to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a6ab52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing of the data\n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from new_helper_1 import *\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "df = pd.read_csv('../data/compas-scores-two-years.csv')\n",
    "features = ['race', 'age', 'sex', 'juv_misd_count', 'priors_count']\n",
    "#features chosen for our C-Logistic Regression\n",
    "to_predict = 'two_year_recid'\n",
    "races_to_filter = ['Caucasian', 'African-American']\n",
    "filtered = df.loc[df['race'].isin(races_to_filter), features + [to_predict]].reset_index(drop=True)\n",
    "\n",
    "#replace categorical data with boolean numbers, 0 and 1\n",
    "filtered['race'] = filtered['race'].apply(lambda race: 0 if race == 'Caucasian' else 1)\n",
    "filtered['sex'] = filtered['sex'].apply(lambda sex: 0 if sex == 'Male' else 1)\n",
    "#x=filtered[['race', 'age', 'sex', 'juv_misd_count', 'priors_count']]\n",
    "#y=filtered[['two_year_recid']]\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#Normalizing the data, so each variable is similar in weight\n",
    "normalized_df = (filtered-filtered.mean())/filtered.std()\n",
    "filtered['age'] = normalized_df['age']\n",
    "filtered['juv_misd_count'] = normalized_df['juv_misd_count']\n",
    "filtered['priors_count'] = normalized_df['priors_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6845a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train_model\n",
    "import loss_funcs\n",
    "\n",
    "train_size = 5000\n",
    "x_train = filtered.loc[:train_size, features]\n",
    "y_train = filtered.loc[:train_size, to_predict]\n",
    "x_control = {'race': x_train['race'].to_list()}\n",
    "\n",
    "apply_fairness_constraints = 1\n",
    "apply_accuracy_constraint = 0\n",
    "sep_constraint = 0\n",
    "gamma = 0\n",
    "sensitive_attrs = ['race']\n",
    "sensitive_attrs_to_cov_thresh = {'race': 0}\n",
    "\n",
    "w = train_model(x_train.to_numpy(),\n",
    "                y_train.to_numpy(),\n",
    "                x_control,\n",
    "                loss_funcs._logistic_loss,\n",
    "                apply_fairness_constraints,\n",
    "                apply_accuracy_constraint,\n",
    "                sep_constraint,\n",
    "                sensitive_attrs,\n",
    "                sensitive_attrs_to_cov_thresh,\n",
    "                gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41da8a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.19550997,  1.7853571 ,  3.76622418,  5.74709125,  7.72795833,\n",
       "         9.7088254 , 11.68969247, 15.65142662, 23.57489492, 25.55576199]),\n",
       " array([4688,  222,   51,   22,    6,    4,    4,    2,    1,    1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(x_train['juv_misd_count'], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a96e77",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "\n",
    "We adapted the code from the repository linked with our paper \"Fairness Constraints: Mechanisms for Fair Classification\"\n",
    "\n",
    "\n",
    "utils, the file that we are importing the train_model function from is also in the doc folder of our repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4babaf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients from training the model are: [24.54716154 -0.06274797 16.72642233 -0.33462877  0.16826255]\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "from utils import train_model\n",
    "import loss_funcs\n",
    "\n",
    "train_size = 5000\n",
    "x_train = filtered.loc[:train_size, features]\n",
    "y_train = filtered.loc[:train_size, to_predict]\n",
    "x_test = filtered.loc[train_size:, features]\n",
    "y_test = filtered.loc[train_size:, to_predict]\n",
    "x_control = {'race': x_train['race'].to_list()}\n",
    "\n",
    "\n",
    "\n",
    "apply_fairness_constraints = 0\n",
    "apply_accuracy_constraint = 0\n",
    "sep_constraint = 0\n",
    "gamma = 0\n",
    "sensitive_attrs = ['race']\n",
    "sensitive_attrs_to_cov_thresh = {'race': 0}\n",
    "\n",
    "#coefficients from training the model\n",
    "w = train_model(x_train.to_numpy(),\n",
    "                y_train.to_numpy(),\n",
    "                x_control,\n",
    "                loss_funcs._logistic_loss,\n",
    "                apply_fairness_constraints,\n",
    "                apply_accuracy_constraint,\n",
    "                sep_constraint,\n",
    "                sensitive_attrs,\n",
    "                sensitive_attrs_to_cov_thresh,\n",
    "                gamma)\n",
    "\n",
    "\n",
    "print(\"The coefficients from training the model are:\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be549646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:: 0.5365217391304348\n"
     ]
    }
   ],
   "source": [
    "#used sklearn to fit these coefficients to a Logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "m = LogisticRegression()\n",
    "m.coef_= w.reshape((1,-1))\n",
    "m.intercept_ = 0\n",
    "m.classes_ = np.array([0, 1])\n",
    "acc = (m.predict(x_test[features]) == y_test).sum() / len(y_test)\n",
    "print(\"Logistic Regression Accuracy::\", acc)\n",
    "\n",
    "#Accuracy of 54% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daceef6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients: [ 2.02536978e+01 -6.27560107e-02  3.46271309e+02 -3.34488239e-01\n",
      "  1.68227794e-01]\n"
     ]
    }
   ],
   "source": [
    "#section applying fairness constraints\n",
    "\n",
    "apply_fairness_constraints = 1\n",
    "apply_accuracy_constraint = 0\n",
    "sep_constraint = 0\n",
    "gamma = 0\n",
    "sensitive_attrs = ['race']\n",
    "sensitive_attrs_to_cov_thresh = {'race': 0}\n",
    "\n",
    "w = train_model(x_train.to_numpy(),\n",
    "                y_train.to_numpy(),\n",
    "                x_control,\n",
    "                loss_funcs._logistic_loss,\n",
    "                apply_fairness_constraints,\n",
    "                apply_accuracy_constraint,\n",
    "                sep_constraint,\n",
    "                sensitive_attrs,\n",
    "                sensitive_attrs_to_cov_thresh,\n",
    "                gamma)\n",
    "\n",
    "print(\"The coefficients:\",w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c422002",
   "metadata": {},
   "source": [
    "### Maximizing accuracy under fairness constraints (C-SVM and C-LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b4859be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as ut\n",
    "import loss_funcs as lf\n",
    "def test_data():\n",
    "    X, y, x_control = filtered\n",
    "    ut.compute_p_rule(x_control[\"sex\"], y) # compute the p-rule in the original data\n",
    "    \n",
    "    \"\"\" Split the data into train and test \"\"\"\n",
    "    X = ut.add_intercept(X) # add intercept to X before applying the linear classifier\n",
    "    train_fold_size = 0.7\n",
    "    x_train, y_train, x_control_train, x_test, y_test, x_control_test = ut.split_into_train_test(X, y, x_control, train_fold_size)\n",
    "    \n",
    "    apply_fairness_constraints = None\n",
    "    apply_accuracy_constraint = None\n",
    "    sep_constraint = None\n",
    "\n",
    "    loss_function = lf._logistic_loss\n",
    "    sensitive_attrs = [\"sex\"]\n",
    "    sensitive_attrs_to_cov_thresh = {}\n",
    "    gamma = None\n",
    "    \n",
    "    def train_test_classifier():\n",
    "        w = ut.train_model(x_train, y_train, x_control_train, loss_function, apply_fairness_constraints, apply_accuracy_constraint, sep_constraint, sensitive_attrs, sensitive_attrs_to_cov_thresh, gamma)\n",
    "        train_score, test_score, correct_answers_train, correct_answers_test = ut.check_accuracy(w, x_train, y_train, x_test, y_test, None, None)\n",
    "        distances_boundary_test = (np.dot(x_test, w)).tolist()\n",
    "        all_class_labels_assigned_test = np.sign(distances_boundary_test)\n",
    "        correlation_dict_test = ut.get_correlations(None, None, all_class_labels_assigned_test, x_control_test, sensitive_attrs)\n",
    "        cov_dict_test = ut.print_covariance_sensitive_attrs(None, x_test, distances_boundary_test, x_control_test, sensitive_attrs)\n",
    "        p_rule = ut.print_classifier_fairness_stats([test_score], [correlation_dict_test], [cov_dict_test], sensitive_attrs[0])\t\n",
    "        return w, p_rule, test_score\n",
    "    \n",
    " \n",
    "    print(\"== Unconstrained (original) classifier ==\")\n",
    "    # all constraint flags are set to 0 since we want to train an unconstrained (original) classifier\n",
    "    apply_fairness_constraints = 0\n",
    "    apply_accuracy_constraint = 0\n",
    "    sep_constraint = 0\n",
    "    w_uncons, p_uncons, acc_uncons = train_test_classifier()\n",
    "    \n",
    "    \"\"\" Now classify such that we optimize for accuracy while achieving perfect fairness \"\"\"\n",
    "    apply_fairness_constraints = 1 # set this flag to one since we want to optimize accuracy subject to fairness constraints\n",
    "    apply_accuracy_constraint = 0\n",
    "    sep_constraint = 0\n",
    "    sensitive_attrs_to_cov_thresh = {\"sex\":0}\n",
    "    print\n",
    "    print(\"== Classifier with fairness constraint ==\")\n",
    "    w_f_cons, p_f_cons, acc_f_cons  = train_test_classifier()\n",
    "    \n",
    "    \n",
    "    \"\"\" Classify such that we optimize for fairness subject to a certain loss in accuracy \"\"\"\n",
    "    apply_fairness_constraints = 0 # flag for fairness constraint is set back to0 since we want to apply the accuracy constraint now\n",
    "    apply_accuracy_constraint = 1 # now, we want to optimize fairness subject to accuracy constraints\n",
    "    sep_constraint = 0\n",
    "    gamma = 0.5 # gamma controls how much loss in accuracy we are willing to incur to achieve fairness -- increase gamme to allow more loss in accuracy\n",
    "    print(\"== Classifier with accuracy constraint ==\")\n",
    "    w_a_cons, p_a_cons, acc_a_cons = train_test_classifier()\t\n",
    "    \n",
    "    \"\"\" \n",
    "    Classify such that we optimize for fairness subject to a certain loss in accuracy \n",
    "    In addition, make sure that no points classified as positive by the unconstrained (original) classifier are misclassified.\n",
    "    \"\"\"\n",
    "    apply_fairness_constraints = 0 # flag for fairness constraint is set back to0 since we want to apply the accuracy constraint now\n",
    "    apply_accuracy_constraint = 1 # now, we want to optimize accuracy subject to fairness constraints\n",
    "    sep_constraint = 1 # set the separate constraint flag to one, since in addition to accuracy constrains, we also want no misclassifications for certain points (details in demo README.md)\n",
    "    gamma = 1000.0\n",
    "    print(\"== Classifier with accuracy constraint (no +ve misclassification) ==\")\n",
    "    w_a_cons_fine, p_a_cons_fine, acc_a_cons_fine  = train_test_classifier()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62753685",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "043d517b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[549 121]\n",
      " [310 250]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.82      0.72       670\n",
      "           1       0.67      0.45      0.54       560\n",
      "\n",
      "    accuracy                           0.65      1230\n",
      "   macro avg       0.66      0.63      0.63      1230\n",
      "weighted avg       0.65      0.65      0.64      1230\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "#Preproccessing\n",
    "\n",
    "features = ['race', 'age', 'sex', 'juv_misd_count', 'priors_count']\n",
    "to_predict = 'two_year_recid'\n",
    "races_to_filter = ['Caucasian', 'African-American']\n",
    "# df.loc[df['race'].isin(races_to_filter), features + [to_predict]]\n",
    "df = df.loc[df['race'].isin(races_to_filter), features + [to_predict]]\n",
    "\n",
    "\n",
    "#transform race and sex into 0 and 1 \n",
    "#African-American will be 0 and Caucasian will be 1\n",
    "#Male will be 0 and Female will be 1\n",
    "\n",
    "df['race'] = df['race'].replace(['African-American'],0)\n",
    "df['race'] = df['race'].replace(['Caucasian'],1)\n",
    "df['sex'] = df['sex'].replace(['Male'],0)\n",
    "df['sex'] = df['sex'].replace(['Female'],1)\n",
    "\n",
    "#normalize age, juv_misd_count, and priors_count\n",
    "\n",
    "normalized_df = (df-df.mean())/df.std()\n",
    "df['age'] = normalized_df['age']\n",
    "df['juv_misd_count'] = normalized_df['juv_misd_count']\n",
    "df['priors_count'] = normalized_df['priors_count']\n",
    "# normalized_df.head()\n",
    "df.head()\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X = df.drop('two_year_recid',axis=1)\n",
    "Y = df['two_year_recid']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20)\n",
    "\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "#Accuracy of .68\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14449fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM with fairness constraints\n",
    "\n",
    "import math\n",
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "import random\n",
    "import SVM_utils as utils\n",
    "#from utils import sign\n",
    "\n",
    "DEFAULT_NUM_ROUNDS = 1\n",
    "DEFAULT_LAMBDA = 1.0\n",
    "DEFAULT_GAMMA = 0.1\n",
    "\n",
    "\n",
    "def hyperplaneToHypothesis(w):\n",
    "   return lambda x: sign(numpy.dot(w,x))\n",
    "\n",
    "\n",
    "# use scikit-learn to do the svm for us\n",
    "def svmDetailedSKL(data, gamma=DEFAULT_GAMMA, verbose=False, kernel='rbf'):\n",
    "  # if verbose:\n",
    "   #  print(\"Loading scikit-learn\")\n",
    "   from sklearn import svm\n",
    "   points, labels = zip(*data)\n",
    "   clf = svm.SVC(kernel=kernel, gamma=gamma)\n",
    "\n",
    "   #if verbose:\n",
    "   #   print(\"Training classifier\")\n",
    "\n",
    "   skClassifier = clf.fit(points, labels)\n",
    "   hypothesis = lambda x: skClassifier.predict([x])[0]\n",
    "   bulkHypothesis = lambda data: skClassifier.predict(data)\n",
    "\n",
    "   alphas = skClassifier.dual_coef_[0]\n",
    "   supportVectors = skClassifier.support_vectors_\n",
    "   error = lambda data: 1 - skClassifier.score(*zip(*data))\n",
    "\n",
    "   intercept = skClassifier.intercept_\n",
    "   margin = lambda y: skClassifier.decision_function([y])[0]\n",
    "   bulkMargin = lambda pts: skClassifier.decision_function(pts)\n",
    "\n",
    "   #if verbose:\n",
    "   #   print(\"Done\")\n",
    "\n",
    "   return (hypothesis, bulkHypothesis, skClassifier, error, alphas, intercept,\n",
    "            gamma, supportVectors, bulkMargin, margin)\n",
    "\n",
    "\n",
    "def svmSKL(data, gamma=DEFAULT_GAMMA, verbose=False, kernel='rbf'):\n",
    "   return svmDetailedSKL(data, gamma, verbose, kernel)[0]\n",
    "\n",
    "def svmLinearSKL(data, verbose=False):\n",
    "   return svmDetailedSKL(data, 0, verbose, 'linear')[0]\n",
    "\n",
    "# compute the margin of a point\n",
    "def margin(point, hyperplane):\n",
    "   return numpy.dot(hyperplane, point)\n",
    "\n",
    "# compute the absolute value of the margin of a point\n",
    "def absMargin(point, hyperplane):\n",
    "   return abs(margin(point, hyperplane))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49acdf0c",
   "metadata": {},
   "source": [
    "### Information Theoretic Measures for Fairness-aware Feature Selection\n",
    "\n",
    "The Fairness-aware Feature Selection (FFS) framework depends on the joint statistics of the data. It utilizes information decomposition to calculate two information-theoretic measures that separately quantify the accuracy and discriminatory impact of every subset of features. \n",
    "\n",
    "Subsequently, based on the two information-theoretic measures of each subset, the authors deduce an accuracy coefficient and a discrimination coefficient for each feature using Shapely-value analysis. The two coefficients capture the marginal impacts on accuracy and discrimination of each feature, respectively.\n",
    "\n",
    "Note that the two coefficients are deduced using the information-theoretic measures for all subsets of features. This allows consideration for the interdependencies among the features. \n",
    "\n",
    "Finally, combining the two coefficients, a fairness-utility score is assigned for each feature, and we can do feature selection based on this score. Itâ€™s worth noting that both the calculation of the fairness-utility scores and the feature selection process rely on personal judgement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4c6dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data \n",
    "\n",
    "fp = '../data/compas-scores-two-years.csv'\n",
    "df = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b098a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karel/Desktop/Fall2021-Project4-group4/doc/new_helper_1.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  compas_subset[\"two_year_recid\"] = compas_subset[\"two_year_recid\"].apply(lambda x: -1 if x==0 else 1)\n",
      "/Users/karel/Desktop/Fall2021-Project4-group4/doc/new_helper_1.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  compas_subset[\"two_year_recid\"] = compas_subset[\"two_year_recid\"].apply(lambda x: -1 if x==0 else 1)\n",
      "/Users/karel/Desktop/Fall2021-Project4-group4/doc/new_helper_1.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  compas_subset[\"two_year_recid\"] = compas_subset[\"two_year_recid\"].apply(lambda x: -1 if x==0 else 1)\n",
      "/Users/karel/Desktop/Fall2021-Project4-group4/doc/new_helper_1.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  compas_subset[\"two_year_recid\"] = compas_subset[\"two_year_recid\"].apply(lambda x: -1 if x==0 else 1)\n",
      "/Users/karel/Desktop/Fall2021-Project4-group4/doc/new_helper_1.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  compas_subset[\"two_year_recid\"] = compas_subset[\"two_year_recid\"].apply(lambda x: -1 if x==0 else 1)\n",
      "/Users/karel/Desktop/Fall2021-Project4-group4/doc/new_helper_1.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  compas_subset[\"two_year_recid\"] = compas_subset[\"two_year_recid\"].apply(lambda x: -1 if x==0 else 1)\n"
     ]
    }
   ],
   "source": [
    "##split data\n",
    "\n",
    "train_set = set_split_train(data_process(df)[0],data_process(df)[1],data_process(df)[2])\n",
    "test_set = set_split_test(data_process(df)[0],data_process(df)[1],data_process(df)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84f799a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Feature  Discrimination  Accuracy\n",
      "0            race    7.174550e+06  2.459747\n",
      "1             age    4.921024e+06  1.366085\n",
      "2             sex    3.070006e+06  1.236041\n",
      "3  juv_misd_count    3.675583e+06  1.311511\n",
      "4    priors_count    4.033772e+06  1.293547\n"
     ]
    }
   ],
   "source": [
    "# Calculation\n",
    "\n",
    "accuracy, discriminate = shapley_Cal(train_set)[0], shapley_Cal(train_set)[1]\n",
    "#print result\n",
    "shapley_print(discriminate,accuracy)\n",
    "\n",
    "# 68%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b58d8968",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-64d6e9c7bf8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_cal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# The final accuracy is 68%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_result' is not defined"
     ]
    }
   ],
   "source": [
    "test_acc,test_cal = test_result(train_set, test_set)  \n",
    "test_print(test_acc)\n",
    "\n",
    "# The final accuracy is 68%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49593c79",
   "metadata": {},
   "source": [
    "We can see that the accuary is different from the baseline model. Not only for the A2 but also for A7. Because of the fact that the accuarcy is improved by considering the fairness. Moreover, Maximizing accuracy under fairness constraints implies that dropping two_year_recid factor is an efficient way to decrease discrimination. And information Theoretic Measures for Fairness-aware Feature selection (FFS)) as it is predicted by the marginal discrimination coefficient, removal of Age or Prior Counts results in the lowest bias in the classifier output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
